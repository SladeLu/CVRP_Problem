{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import multiprocessing as mp\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "from envs.test_env_v2 import TestEnv_v2\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 64\n",
    "PERCENTILE = 70\n",
    "LEARNING_RATE = 0.0005\n",
    "REUSE_TIMES = 6\n",
    "GAMMA = 1.001\n",
    "\n",
    "USE_CORES = 8\n",
    "\n",
    "INIT_ROUNDS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size[1], hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, int(hidden_size/2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.out = nn.Linear(obs_size[0] * int(hidden_size/2), n_actions) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = x.view(x.size(0), -1)   # to (batch_size, obs_size[0] * hidden_size/2)\n",
    "        output = self.out(x)\n",
    "        return output\n",
    "\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps', 'info'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "global total_batch\n",
    "def produce_batches(env_input, net, batch_size):\n",
    "    env = copy.deepcopy(env_input)\n",
    "    np.random.seed()\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    times = 0\n",
    "    \n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs]).cuda()\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.cpu().data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, ext_info = env.step(action)\n",
    "        episode_reward += reward * (GAMMA ** len(episode_steps))\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        if is_done or ext_info[3] > 10000:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps, info=ext_info))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "\n",
    "#             print(\"%d\"% (times%8), end='', flush=True)\n",
    "#             times += 1\n",
    "#             if times%8 == 0:\n",
    "#                 print(\"|\", end='')\n",
    "\n",
    "            if len(batch) == batch_size:\n",
    "#                 print(\" \")\n",
    "                times = 0\n",
    "                del obs_v, act_probs_v, act_probs, ext_info\n",
    "                return batch\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "def collect_results(mini_batch):\n",
    "    total_batch.extend(mini_batch)\n",
    "\n",
    "def apply_async_with_callback(pool, core_num, env, net, batch_size):\n",
    "    core_thrd = []\n",
    "    for _ in range(core_num):\n",
    "        core_thrd.append(pool.apply_async(produce_batches, args=(env, net, int(batch_size/core_num)),\n",
    "                         callback=collect_results))\n",
    "    for i in range(core_num):\n",
    "        core_thrd[i].get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "    infos = np.array(list(map(lambda s: s.info, batch)))\n",
    "    info_mean = np.mean(infos,axis=0)\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    elite_batch = []\n",
    "    for example, discounted_reward in zip(batch, rewards):\n",
    "        if discounted_reward > reward_bound:\n",
    "            train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "            train_act.extend(map(lambda step: step.action, example.steps))\n",
    "            elite_batch.append(example)\n",
    "\n",
    "    return elite_batch, train_obs, train_act, reward_bound, reward_mean, info_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_init_batch(env,batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    r = 0\n",
    "    test_num = 0\n",
    "    times = 0\n",
    "    while True:\n",
    "        r = random.randint(0,99)\n",
    "        next_obs, reward, is_done,  ext_info = env.step(r)\n",
    "        episode_reward += reward * (GAMMA ** len(episode_steps))\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=r))\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps, info=ext_info))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if times%8 == 0:\n",
    "                print(\"|\", end='')\n",
    "            print(\"%d\"% (times%8), end='',flush=True)\n",
    "            times += 1\n",
    "            if len(batch) == batch_size:\n",
    "                print(\" \")\n",
    "                yield batch\n",
    "                batch = []\n",
    "                test_num +=1\n",
    "                times = 0\n",
    "                if test_num == INIT_ROUNDS:\n",
    "                    break\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 4 GPUs!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = TestEnv_v2()\n",
    "    # env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "    # os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "    # os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "    \n",
    "    \n",
    "    # print(device_lib.list_local_devices())\n",
    "    \n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    pool = mp.Pool(processes = USE_CORES)\n",
    "    \n",
    "    \n",
    "    obs_size = env.observation_size\n",
    "    n_actions = env.action_num\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        net = nn.DataParallel(net)\n",
    "\n",
    "    net = net.cuda()\n",
    "    \n",
    "    objective = nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=LEARNING_RATE)\n",
    "    writer = SummaryWriter(comment=\"-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training!!!\n",
      "1: reward_mean=-12284.9, reward_bound=-8278.5, round_mean=32.9,     distance_mean=79.8, steps=1131 \n",
      "------ \n",
      "iter time: 53 s, localtime: Wed Nov 21 14:01:34 2018\n",
      "2: reward_mean=-10592.0, reward_bound=-6954.4, round_mean=33.5,     distance_mean=80.6, steps=1052 \n",
      "------ \n",
      "iter time: 30 s, localtime: Wed Nov 21 14:02:18 2018\n",
      "3: reward_mean=-10471.7, reward_bound=-6807.6, round_mean=32.8,     distance_mean=80.3, steps=1050 \n",
      "------ \n",
      "iter time: 31 s, localtime: Wed Nov 21 14:02:49 2018\n",
      "4: reward_mean=-10735.3, reward_bound=-6732.6, round_mean=32.5,     distance_mean=80.0, steps=1052 \n",
      "------ \n",
      "iter time: 31 s, localtime: Wed Nov 21 14:03:21 2018\n",
      "5: reward_mean=-10629.0, reward_bound=-6436.6, round_mean=33.5,     distance_mean=80.6, steps=1047 \n",
      "------ \n",
      "iter time: 31 s, localtime: Wed Nov 21 14:03:52 2018\n",
      "6: reward_mean=-10736.5, reward_bound=-6237.1, round_mean=33.1,     distance_mean=79.9, steps=1044 \n",
      "------ \n",
      "iter time: 30 s, localtime: Wed Nov 21 14:04:23 2018\n",
      "7: reward_mean=-12260.1, reward_bound=-6118.2, round_mean=33.3,     distance_mean=80.7, steps=1075 \n",
      "------ \n",
      "iter time: 32 s, localtime: Wed Nov 21 14:04:56 2018\n",
      "8: reward_mean=-10419.0, reward_bound=-5946.9, round_mean=32.9,     distance_mean=79.6, steps=1038 \n",
      "------ \n",
      "iter time: 32 s, localtime: Wed Nov 21 14:05:28 2018\n",
      "9: reward_mean=-9651.3, reward_bound=-5886.6, round_mean=32.8,     distance_mean=79.9, steps=999 \n",
      "------ \n",
      "iter time: 31 s, localtime: Wed Nov 21 14:05:59 2018\n",
      "10: reward_mean=-11341.1, reward_bound=-5822.1, round_mean=33.5,     distance_mean=80.4, steps=1047 \n",
      "------ \n",
      "iter time: 32 s, localtime: Wed Nov 21 14:06:31 2018\n",
      "11: reward_mean=-10745.6, reward_bound=-5689.9, round_mean=33.3,     distance_mean=80.2, steps=1038 \n",
      "------ \n",
      "iter time: 30 s, localtime: Wed Nov 21 14:07:02 2018\n",
      "12: reward_mean=-10001.2, reward_bound=-5650.6, round_mean=32.5,     distance_mean=79.6, steps=1021 \n",
      "------ \n",
      "iter time: 30 s, localtime: Wed Nov 21 14:07:33 2018\n",
      "13: reward_mean=-11366.1, reward_bound=-5842.9, round_mean=33.5,     distance_mean=80.2, steps=1063 \n",
      "------ \n",
      "iter time: 33 s, localtime: Wed Nov 21 14:08:06 2018\n",
      "14: reward_mean=-10258.8, reward_bound=-5571.8, round_mean=33.2,     distance_mean=80.3, steps=1016 \n",
      "------ \n",
      "iter time: 31 s, localtime: Wed Nov 21 14:08:38 2018\n",
      "15: reward_mean=-10280.3, reward_bound=-5550.2, round_mean=33.2,     distance_mean=80.1, steps=1032 \n",
      "------ \n",
      "iter time: 31 s, localtime: Wed Nov 21 14:09:09 2018\n",
      "16: reward_mean=-9814.9, reward_bound=-5480.7, round_mean=32.9,     distance_mean=79.9, steps=994 \n",
      "------ \n",
      "iter time: 30 s, localtime: Wed Nov 21 14:09:39 2018\n",
      "17: reward_mean=-10426.0, reward_bound=-5413.0, round_mean=33.2,     distance_mean=80.1, steps=1024 \n",
      "------ \n",
      "iter time: 31 s, localtime: Wed Nov 21 14:10:11 2018\n",
      "18: reward_mean=-9759.4, reward_bound=-5425.1, round_mean=33.2,     distance_mean=80.1, steps=1002 \n",
      "------ \n",
      "iter time: 31 s, localtime: Wed Nov 21 14:10:43 2018\n",
      "19: reward_mean=-9475.0, reward_bound=-5179.6, round_mean=32.6,     distance_mean=79.4, steps=988 \n",
      "------ \n",
      "iter time: 30 s, localtime: Wed Nov 21 14:11:13 2018\n",
      "20: reward_mean=-11034.5, reward_bound=-5150.5, round_mean=33.6,     distance_mean=80.4, steps=1035 \n",
      "------ \n",
      "iter time: 32 s, localtime: Wed Nov 21 14:11:45 2018\n",
      "21: reward_mean=-11702.2, reward_bound=-5120.9, round_mean=33.8,     distance_mean=80.4, steps=1073 \n",
      "------ \n",
      "iter time: 33 s, localtime: Wed Nov 21 14:12:20 2018\n",
      "22: reward_mean=-9870.1, reward_bound=-5044.5, round_mean=33.4,     distance_mean=79.9, steps=992 \n",
      "------ \n",
      "iter time: 30 s, localtime: Wed Nov 21 14:12:50 2018\n",
      "23: reward_mean=-10341.4, reward_bound=-5054.4, round_mean=33.2,     distance_mean=80.2, steps=1023 \n",
      "------ \n",
      "iter time: 30 s, localtime: Wed Nov 21 14:13:21 2018\n",
      "24: reward_mean=-11044.5, reward_bound=-5023.3, round_mean=33.5,     distance_mean=80.5, steps=1032 \n",
      "------ \n",
      "iter time: 31 s, localtime: Wed Nov 21 14:13:52 2018\n",
      "25: reward_mean=-9850.6, reward_bound=-4908.9, round_mean=32.6,     distance_mean=79.1, steps=997 \n",
      "------ \n",
      "iter time: 31 s, localtime: Wed Nov 21 14:14:24 2018\n",
      "26: reward_mean=-9790.5, reward_bound=-5133.6, round_mean=33.3,     distance_mean=79.9, steps=1003 \n",
      "------ \n",
      "iter time: 30 s, localtime: Wed Nov 21 14:14:54 2018\n",
      "27: reward_mean=-9833.2, reward_bound=-4847.2, round_mean=33.3,     distance_mean=80.2, steps=994 \n",
      "------ \n",
      "iter time: 30 s, localtime: Wed Nov 21 14:15:25 2018\n",
      "28: reward_mean=-9513.8, reward_bound=-4812.2, round_mean=33.3,     distance_mean=80.2, steps=983 \n",
      "------ \n",
      "iter time: 30 s, localtime: Wed Nov 21 14:15:55 2018\n",
      "29: reward_mean=-9700.5, reward_bound=-4764.2, round_mean=33.3,     distance_mean=80.5, steps=992 \n",
      "------ \n",
      "iter time: 30 s, localtime: Wed Nov 21 14:16:26 2018\n",
      "30: reward_mean=-9840.7, reward_bound=-4819.9, round_mean=33.7,     distance_mean=80.8, steps=997 \n",
      "------ \n",
      "iter time: 30 s, localtime: Wed Nov 21 14:16:56 2018\n",
      "31: reward_mean=-9932.4, reward_bound=-4834.5, round_mean=33.6,     distance_mean=80.6, steps=1004 \n",
      "------ \n",
      "iter time: 30 s, localtime: Wed Nov 21 14:17:27 2018\n",
      "32: reward_mean=-9663.3, reward_bound=-4707.6, round_mean=33.1,     distance_mean=79.8, steps=972 \n",
      "------ \n",
      "iter time: 29 s, localtime: Wed Nov 21 14:17:56 2018\n",
      "33: reward_mean=-9029.9, reward_bound=-4799.3, round_mean=33.6,     distance_mean=80.7, steps=944 \n",
      "------ \n",
      "iter time: 28 s, localtime: Wed Nov 21 14:18:25 2018\n",
      "34: reward_mean=-10083.0, reward_bound=-5056.6, round_mean=33.2,     distance_mean=79.6, steps=1009 \n",
      "------ \n",
      "iter time: 30 s, localtime: Wed Nov 21 14:18:56 2018\n",
      "35: reward_mean=-10290.0, reward_bound=-4685.2, round_mean=33.5,     distance_mean=80.2, steps=1006 \n",
      "------ \n",
      "iter time: 30 s, localtime: Wed Nov 21 14:19:27 2018\n",
      "36: reward_mean=-9671.5, reward_bound=-4668.0, round_mean=34.2,     distance_mean=81.4, steps=980 \n",
      "------ \n",
      "iter time: 30 s, localtime: Wed Nov 21 14:19:58 2018\n",
      "37: reward_mean=-9435.3, reward_bound=-4656.5, round_mean=33.8,     distance_mean=80.5, steps=970 \n",
      "------ \n",
      "iter time: 29 s, localtime: Wed Nov 21 14:20:27 2018\n",
      "38: reward_mean=-10383.2, reward_bound=-5006.9, round_mean=33.7,     distance_mean=80.9, steps=1009 \n",
      "------ \n",
      "iter time: 31 s, localtime: Wed Nov 21 14:20:59 2018\n",
      "39: reward_mean=-9583.7, reward_bound=-4642.8, round_mean=33.5,     distance_mean=80.6, steps=985 \n",
      "------ \n",
      "iter time: 29 s, localtime: Wed Nov 21 14:21:28 2018\n",
      "40: reward_mean=-11051.3, reward_bound=-4619.0, round_mean=33.3,     distance_mean=80.1, steps=1041 \n",
      "------ \n",
      "iter time: 31 s, localtime: Wed Nov 21 14:22:00 2018\n",
      "41: reward_mean=-8781.6, reward_bound=-4697.1, round_mean=33.4,     distance_mean=80.0, steps=942 \n",
      "------ \n",
      "iter time: 28 s, localtime: Wed Nov 21 14:22:29 2018\n",
      "42: reward_mean=-9606.8, reward_bound=-4638.7, round_mean=33.0,     distance_mean=80.2, steps=979 \n",
      "------ \n",
      "iter time: 29 s, localtime: Wed Nov 21 14:22:59 2018\n",
      "43: reward_mean=-10794.7, reward_bound=-4598.2, round_mean=33.2,     distance_mean=80.0, steps=1027 \n",
      "------ \n",
      "iter time: 31 s, localtime: Wed Nov 21 14:23:30 2018\n",
      "44: reward_mean=-10507.6, reward_bound=-4693.1, round_mean=33.3,     distance_mean=80.2, steps=1006 \n",
      "------ \n",
      "iter time: 31 s, localtime: Wed Nov 21 14:24:01 2018\n",
      "45: reward_mean=-9360.9, reward_bound=-4642.6, round_mean=33.1,     distance_mean=80.1, steps=959 \n",
      "------ \n",
      "iter time: 29 s, localtime: Wed Nov 21 14:24:31 2018\n",
      "46: reward_mean=-10738.2, reward_bound=-4660.9, round_mean=33.7,     distance_mean=80.4, steps=1028 \n",
      "------ \n",
      "iter time: 31 s, localtime: Wed Nov 21 14:25:02 2018\n",
      "47: reward_mean=-9485.7, reward_bound=-4572.3, round_mean=33.1,     distance_mean=79.7, steps=968 \n",
      "------ \n",
      "iter time: 30 s, localtime: Wed Nov 21 14:25:33 2018\n",
      "48: reward_mean=-10264.1, reward_bound=-4553.3, round_mean=33.6,     distance_mean=80.4, steps=1007 \n",
      "------ \n",
      "iter time: 30 s, localtime: Wed Nov 21 14:26:04 2018\n"
     ]
    }
   ],
   "source": [
    "    writer = SummaryWriter(comment=\"-test\")\n",
    "    \n",
    "    # net.load_state_dict(torch.load('net_params_init.pkl'))\n",
    "    net.load_state_dict(torch.load('net_params.pkl'))\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    print(\"start training!!!\")\n",
    "    start = time.time()\n",
    "    \n",
    "    res_batch = []\n",
    "    iter_no = 0\n",
    "    while True:\n",
    "        total_batch = []\n",
    "        apply_async_with_callback(pool, USE_CORES, env, net, BATCH_SIZE)\n",
    "        \n",
    "        reward_mean = float(np.mean(list(map(lambda s: s.reward, total_batch))))\n",
    "        res_batch, obs, acts, reward_b, reward_m, info_m = filter_batch(res_batch + total_batch, PERCENTILE)\n",
    "        if not res_batch:\n",
    "            continue\n",
    "        \n",
    "        iter_no += 1\n",
    "        \n",
    "        localtime = time.asctime( time.localtime(time.time()) )\n",
    "        print(\"%d: reward_mean=%.1f, reward_bound=%.1f, round_mean=%.1f, \\\n",
    "        distance_mean=%.1f, steps=%d \"% \\\n",
    "              (iter_no,  reward_m, reward_b, info_m[1], info_m[2], info_m[3]))\n",
    "        \n",
    "        \n",
    "        obs_v = torch.FloatTensor(obs).cuda()\n",
    "        acts_v = torch.LongTensor(acts).cuda()\n",
    "        res_batch = res_batch[-32:]\n",
    "\n",
    "        for i in range(REUSE_TIMES):\n",
    "            optimizer.zero_grad()\n",
    "            action_scores_v = net(obs_v)\n",
    "            loss_v = objective(action_scores_v, acts_v)\n",
    "            loss_v.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            print(\"-\", end='')\n",
    "        print(\" \")\n",
    "        if iter_no%10 == 0:\n",
    "            torch.save(net.state_dict(), 'net_params.pkl')   \n",
    "            \n",
    "        end = time.time()\n",
    "        print(\"iter time: %d s, localtime:\"% (int(end - start)), localtime)\n",
    "        start = end    \n",
    "        \n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "        writer.add_scalar(\"round_mean\", info_m[1], iter_no)\n",
    "        writer.add_scalar(\"distance_mean\", info_m[2], iter_no)\n",
    "        writer.add_scalar(\"step_mean\", info_m[3], iter_no)\n",
    "        # if reward_m > 500:\n",
    "        #     print(\"Solved!\")\n",
    "        #     break\n",
    "        \n",
    "        del loss_v, acts_v, obs_v, action_scores_v, info_m, reward_m\n",
    "        torch.cuda.empty_cache()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.001)\n",
    "    \n",
    "    for batch in get_init_batch(env, 32):\n",
    "        batch, obs, acts, reward_b, reward_m, info_m = filter_batch(batch, 30)\n",
    "\n",
    "        obs_v = torch.FloatTensor(obs).cuda()\n",
    "        acts_v = torch.LongTensor(acts).cuda()\n",
    "\n",
    "        for i in range(5):\n",
    "            optimizer.zero_grad()\n",
    "            action_scores_v = net(obs_v)\n",
    "            loss_v = objective(action_scores_v, acts_v)\n",
    "            loss_v.backward()\n",
    "            optimizer.step()\n",
    "            print(\"-\", end='')\n",
    "        print(\" \")\n",
    "            \n",
    "        print(\"reward_mean=%.1f, reward_bound=%.1f, round_mean=%.1f, \\\n",
    "        distance_mean=%.1f, steps=%d \"% \\\n",
    "          (reward_m, reward_b, info_m[1], info_m[2], info_m[3]))\n",
    "        del loss_v, acts_v, obs_v, action_scores_v, info_m, reward_m\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    torch.save(net.state_dict(), 'net_params_init.pkl')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
